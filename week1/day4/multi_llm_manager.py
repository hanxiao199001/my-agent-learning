"""
å¤š LLM ç®¡ç†å™¨ - æ¨¡æ‹Ÿ BettaFish æ¶æ„
å­¦ä¹ ç›®æ ‡:
1. ç®¡ç†å¤šä¸ªä¸åŒçš„ LLM API
2. å®ç°è‡ªåŠ¨é™çº§å’Œé‡è¯•
3. æˆæœ¬è¿½è¸ªå’Œä¼˜åŒ–
4. ä¸ºä¸åŒä»»åŠ¡é€‰æ‹©æœ€ä½³æ¨¡å‹
"""

import os
from openai import OpenAI
from dotenv import load_dotenv
from typing import Dict, Optional, Literal
import time
from datetime import datetime

load_dotenv()

# ========== LLM å®¢æˆ·ç«¯å°è£… ==========

class LLMClient:
    """LLM å®¢æˆ·ç«¯åŸºç±»"""
    
    def __init__(self, name: str, api_key: str, base_url: str, model: str):
        self.name = name
        self.model = model
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.call_count = 0
        self.total_tokens = 0
        
    def chat(self, messages: list, temperature: float = 0.7, max_retries: int = 3):
        """è°ƒç”¨ LLM"""
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=temperature
                )
                
                # ç»Ÿè®¡
                self.call_count += 1
                if hasattr(response, 'usage'):
                    self.total_tokens += response.usage.total_tokens
                
                return response.choices[0].message.content
                
            except Exception as e:
                print(f"âŒ [{self.name}] è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                if attempt == max_retries - 1:
                    raise
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
        
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "name": self.name,
            "calls": self.call_count,
            "tokens": self.total_tokens
        }

# ========== ä¸“ä¸šåŒ– LLM å®¢æˆ·ç«¯ ==========

class InsightLLM(LLMClient):
    """Insight Engine - æ•°æ®åˆ†æä¸“ç”¨ (Kimi)"""
    
    def __init__(self):
        # å¦‚æœæ²¡æœ‰ Kimi,å…ˆç”¨ DeepSeek ä»£æ›¿
        api_key = os.getenv("INSIGHT_ENGINE_API_KEY") or os.getenv("DEEPSEEK_API_KEY")
        base_url = os.getenv("INSIGHT_ENGINE_BASE_URL", "https://api.deepseek.com")
        model = os.getenv("INSIGHT_ENGINE_MODEL_NAME", "deepseek-chat")
        
        super().__init__("Insight(Kimi)", api_key, base_url, model)
        self.specialty = "æ•°æ®åˆ†æã€SQLç”Ÿæˆã€ç»Ÿè®¡æ¨ç†"

class MediaLLM(LLMClient):
    """Media Engine - å¤šæ¨¡æ€åˆ†æ (Gemini)"""
    
    def __init__(self):
        # å¦‚æœæ²¡æœ‰ Gemini,ç”¨ DeepSeek ä»£æ›¿
        api_key = os.getenv("MEDIA_ENGINE_API_KEY") or os.getenv("DEEPSEEK_API_KEY")
        base_url = os.getenv("MEDIA_ENGINE_BASE_URL", "https://api.deepseek.com")
        model = os.getenv("MEDIA_ENGINE_MODEL_NAME", "deepseek-chat")
        
        super().__init__("Media(Gemini)", api_key, base_url, model)
        self.specialty = "å¤šæ¨¡æ€ç†è§£ã€å›¾æ–‡åˆ†æ"

class QueryLLM(LLMClient):
    """Query Engine - æœç´¢å’Œæ¨ç† (DeepSeek)"""
    
    def __init__(self):
        api_key = os.getenv("QUERY_ENGINE_API_KEY") or os.getenv("DEEPSEEK_API_KEY")
        base_url = os.getenv("QUERY_ENGINE_BASE_URL", "https://api.deepseek.com")
        model = os.getenv("QUERY_ENGINE_MODEL_NAME", "deepseek-chat")
        
        super().__init__("Query(DeepSeek)", api_key, base_url, model)
        self.specialty = "æ·±åº¦æ¨ç†ã€é€»è¾‘åˆ†æ"

class ReportLLM(LLMClient):
    """Report Engine - æŠ¥å‘Šç”Ÿæˆ (Gemini)"""
    
    def __init__(self):
        api_key = os.getenv("REPORT_ENGINE_API_KEY") or os.getenv("DEEPSEEK_API_KEY")
        base_url = os.getenv("REPORT_ENGINE_BASE_URL", "https://api.deepseek.com")
        model = os.getenv("REPORT_ENGINE_MODEL_NAME", "deepseek-chat")
        
        super().__init__("Report(Gemini)", api_key, base_url, model)
        self.specialty = "å†…å®¹ç”Ÿæˆã€æŠ¥å‘Šæ’°å†™"

class ForumLLM(LLMClient):
    """Forum Host - åè°ƒå’Œç»¼åˆ (Qwen)"""
    
    def __init__(self):
        api_key = os.getenv("FORUM_HOST_API_KEY") or os.getenv("DEEPSEEK_API_KEY")
        base_url = os.getenv("FORUM_HOST_BASE_URL", "https://api.deepseek.com")
        model = os.getenv("FORUM_HOST_MODEL_NAME", "deepseek-chat")
        
        super().__init__("Forum(Qwen)", api_key, base_url, model)
        self.specialty = "åè°ƒç»¼åˆã€å†²çªè§£å†³"

# ========== LLM ç®¡ç†å™¨ ==========

class MultiLLMManager:
    """
    å¤š LLM ç®¡ç†å™¨
    æ¨¡æ‹Ÿ BettaFish çš„å¤šæ¨¡å‹æ¶æ„
    """
    
    def __init__(self):
        print("ğŸš€ åˆå§‹åŒ–å¤š LLM ç®¡ç†å™¨...")
        
        # åˆå§‹åŒ–æ‰€æœ‰ LLM
        self.llms = {
            'insight': InsightLLM(),
            'media': MediaLLM(),
            'query': QueryLLM(),
            'report': ReportLLM(),
            'forum': ForumLLM()
        }
        
        print("âœ… 5 ä¸ªä¸“ä¸š LLM å·²å°±ç»ª\n")
        for name, llm in self.llms.items():
            print(f"   ğŸ“Œ {llm.name}: {llm.specialty}")
    
    def call_agent(
        self, 
        agent_type: Literal['insight', 'media', 'query', 'report', 'forum'],
        task: str,
        context: str = "",
        temperature: float = 0.7
    ) -> str:
        """
        è°ƒç”¨æŒ‡å®š Agent çš„ LLM
        
        Args:
            agent_type: Agent ç±»å‹
            task: ä»»åŠ¡æè¿°
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            temperature: æ¸©åº¦å‚æ•°
        """
        llm = self.llms[agent_type]
        
        # æ„å»ºä¸“ä¸šåŒ–æç¤º
        system_prompt = f"""ä½ æ˜¯ BettaFish ç³»ç»Ÿçš„ {llm.name}ã€‚

ä½ çš„ä¸“é•¿: {llm.specialty}

è¯·æ ¹æ®ä½ çš„ä¸“ä¸šèƒ½åŠ›å®Œæˆä»»åŠ¡ã€‚"""
        
        messages = [
            {"role": "system", "content": system_prompt}
        ]
        
        if context:
            messages.append({"role": "user", "content": f"èƒŒæ™¯ä¿¡æ¯:\n{context}"})
        
        messages.append({"role": "user", "content": f"ä»»åŠ¡:\n{task}"})
        
        print(f"\nğŸ¤– è°ƒç”¨ [{llm.name}]")
        print(f"   ä»»åŠ¡: {task[:50]}...")
        
        result = llm.chat(messages, temperature)
        
        print(f"   âœ… å®Œæˆ")
        
        return result
    
    def parallel_analysis(self, topic: str) -> Dict[str, str]:
        """
        å¹¶è¡Œåˆ†æ - å¤šä¸ª Agent åŒæ—¶å·¥ä½œ
        æ¨¡æ‹Ÿ BettaFish çš„å¹¶è¡Œæ¶æ„
        """
        print(f"\n{'='*60}")
        print(f"ğŸ”„ å¹¶è¡Œåˆ†æä¸»é¢˜: {topic}")
        print(f"{'='*60}")
        
        results = {}
        
        # Agent 1: Insight - æ•°æ®è§†è§’
        results['insight'] = self.call_agent(
            'insight',
            f"ä»æ•°æ®åˆ†æè§’åº¦,åˆ†æä¸»é¢˜: {topic}",
            temperature=0.3
        )
        
        # Agent 2: Query - æœç´¢è§†è§’
        results['query'] = self.call_agent(
            'query',
            f"ä»ä¿¡æ¯æ£€ç´¢è§’åº¦,éœ€è¦æœç´¢ä»€ä¹ˆæ¥äº†è§£: {topic}",
            temperature=0.5
        )
        
        # Agent 3: Media - å†…å®¹è§†è§’
        results['media'] = self.call_agent(
            'media',
            f"ä»å†…å®¹åˆ†æè§’åº¦,è¿™ä¸ªä¸»é¢˜çš„å…³é”®è¦ç´ : {topic}",
            temperature=0.7
        )
        
        return results
    
    def forum_synthesis(self, agent_results: Dict[str, str]) -> str:
        """
        Forum ç»¼åˆ - æ•´åˆå¤šä¸ª Agent çš„ç»“æœ
        æ¨¡æ‹Ÿ BettaFish çš„ Forum Host
        """
        print(f"\n{'='*60}")
        print(f"ğŸ¯ Forum Host ç»¼åˆåˆ†æ")
        print(f"{'='*60}")
        
        # æ„å»ºç»¼åˆæç¤º
        synthesis_prompt = "è¯·ç»¼åˆä»¥ä¸‹ä¸‰ä¸ªä¸“ä¸š Agent çš„åˆ†æ,ç»™å‡ºå®Œæ•´ç»“è®º:\n\n"
        
        for agent_name, result in agent_results.items():
            synthesis_prompt += f"ã€{agent_name.upper()} Agent åˆ†æã€‘:\n{result}\n\n"
        
        synthesis_prompt += "è¯·æ•´åˆä»¥ä¸Šè§‚ç‚¹,è¯†åˆ«å…±è¯†å’Œå·®å¼‚,ç»™å‡ºç»¼åˆç»“è®ºã€‚"
        
        final_result = self.call_agent(
            'forum',
            synthesis_prompt,
            temperature=0.5
        )
        
        return final_result
    
    def generate_report(self, synthesis: str, topic: str) -> str:
        """
        ç”ŸæˆæŠ¥å‘Š - Report Engine
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
        print(f"{'='*60}")
        
        report = self.call_agent(
            'report',
            f"åŸºäºä»¥ä¸‹ç»¼åˆåˆ†æ,æ’°å†™ä¸€ä»½å…³äº'{topic}'çš„ç®€çŸ­æŠ¥å‘Š:\n\n{synthesis}",
            temperature=0.6
        )
        
        return report
    
    def print_statistics(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n{'='*60}")
        print("ğŸ“Š LLM ä½¿ç”¨ç»Ÿè®¡")
        print(f"{'='*60}")
        
        total_calls = 0
        total_tokens = 0
        
        for name, llm in self.llms.items():
            stats = llm.get_stats()
            total_calls += stats['calls']
            total_tokens += stats['tokens']
            print(f"{stats['name']:20} | è°ƒç”¨: {stats['calls']:3} æ¬¡ | Tokens: {stats['tokens']:6}")
        
        print(f"{'-'*60}")
        print(f"{'æ€»è®¡':20} | è°ƒç”¨: {total_calls:3} æ¬¡ | Tokens: {total_tokens:6}")
        print(f"{'='*60}")

# ========== æµ‹è¯•ç¤ºä¾‹ ==========

def demo_bettafish_workflow():
    """
    æ¼”ç¤ºå®Œæ•´çš„ BettaFish å·¥ä½œæµ
    """
    # åˆå§‹åŒ–
    manager = MultiLLMManager()
    
    # åˆ†æä¸»é¢˜
    topic = "2024å¹´è¯ºè´å°”ç‰©ç†å­¦å¥–çš„æ„ä¹‰"
    
    # é˜¶æ®µ1: å¹¶è¡Œåˆ†æ
    agent_results = manager.parallel_analysis(topic)
    
    # é˜¶æ®µ2: Forum ç»¼åˆ
    synthesis = manager.forum_synthesis(agent_results)
    
    # é˜¶æ®µ3: ç”ŸæˆæŠ¥å‘Š
    report = manager.generate_report(synthesis, topic)
    
    # å±•ç¤ºç»“æœ
    print(f"\n{'='*60}")
    print("ğŸ¯ æœ€ç»ˆæŠ¥å‘Š")
    print(f"{'='*60}\n")
    print(report)
    
    # ç»Ÿè®¡
    manager.print_statistics()

if __name__ == "__main__":
    demo_bettafish_workflow()